{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "cUgQLyqL8Xxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO5CMLIf8ag3",
        "outputId": "dd356da2-a00a-4a8d-e03e-0feec96249f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-4523ec4b5e2b>:2: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_df = pd.read_csv('/content/train.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean data\n",
        "def clean_data(df):\n",
        "    # Convert columns to appropriate data types\n",
        "    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
        "    df['Annual_Income'] = pd.to_numeric(df['Annual_Income'], errors='coerce')\n",
        "    df['Outstanding_Debt'] = pd.to_numeric(df['Outstanding_Debt'], errors='coerce')\n",
        "    df['Monthly_Balance'] = pd.to_numeric(df['Monthly_Balance'], errors='coerce')\n",
        "    df['Num_of_Delayed_Payment'] = pd.to_numeric(df['Num_of_Delayed_Payment'], errors='coerce')\n",
        "    df['Changed_Credit_Limit'] = pd.to_numeric(df['Changed_Credit_Limit'], errors='coerce')\n",
        "    df['Amount_invested_monthly'] = pd.to_numeric(df['Amount_invested_monthly'], errors='coerce')\n",
        "\n",
        "    # Handle missing values\n",
        "    df['Monthly_Inhand_Salary'].fillna(df['Monthly_Inhand_Salary'].median(), inplace=True)\n",
        "    df['Num_Credit_Inquiries'].fillna(df['Num_Credit_Inquiries'].median(), inplace=True)\n",
        "    df['Amount_invested_monthly'].fillna(df['Amount_invested_monthly'].median(), inplace=True)\n",
        "    df['Num_of_Delayed_Payment'].fillna(df['Num_of_Delayed_Payment'].median(), inplace=True)\n",
        "    df['Monthly_Balance'].fillna(df['Monthly_Balance'].median(), inplace=True)\n",
        "    df['Credit_History_Age'] = df['Credit_History_Age'].str.extract('(\\d+)').astype(float).fillna(df['Credit_History_Age'].str.extract('(\\d+)').astype(float).median())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "2BNaOHUJ8dQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the datasets\n",
        "train_df_cleaned = clean_data(train_df)\n",
        "test_df_cleaned = clean_data(test_df)\n",
        "\n",
        "# Extracting features and target from training data\n",
        "X_train = train_df_cleaned.drop(columns=['ID', 'Customer_ID', 'Name', 'SSN', 'Credit_Score', 'Month'])\n",
        "y_train = train_df_cleaned['Credit_Score']"
      ],
      "metadata": {
        "id": "f63D0Yyz8hTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify high cardinality features (more than 20 unique categories)\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "high_cardinality_features = [col for col in categorical_features if X_train[col].nunique() > 20]\n",
        "low_cardinality_features = [col for col in categorical_features if X_train[col].nunique() <= 20]\n",
        "\n",
        "# Label encode high cardinality features\n",
        "label_encoders = {col: LabelEncoder().fit(X_train[col]) for col in high_cardinality_features}\n",
        "for col, encoder in label_encoders.items():\n",
        "    X_train[col] = encoder.transform(X_train[col])"
      ],
      "metadata": {
        "id": "2h1Jh1A58lT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode low cardinality features\n",
        "one_hot_encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "X_train_encoded_low_cardinality = one_hot_encoder.fit_transform(X_train[low_cardinality_features])\n",
        "\n",
        "# Convert the encoded features to a DataFrame and concatenate with the rest of the data\n",
        "X_train_encoded_low_cardinality_df = pd.DataFrame(X_train_encoded_low_cardinality, columns=one_hot_encoder.get_feature_names_out(low_cardinality_features))\n",
        "X_train = pd.concat([X_train.drop(columns=low_cardinality_features).reset_index(drop=True), X_train_encoded_low_cardinality_df.reset_index(drop=True)], axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XmVZxlE8qkJ",
        "outputId": "dc59b423-3f2f-438b-caf6-8a863e33f4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure no missing values are left\n",
        "X_train.fillna(X_train.median(), inplace=True)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "\n",
        "# Splitting the data into training and validation sets\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train_normalized, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aJSM4YI18ttg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Train a logistic regression model\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = logistic_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_val, logistic_model.predict_proba(X_val), multi_class='ovr')\n",
        "\n",
        "# Show the evaluation metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')\n",
        "print(f'ROC-AUC: {roc_auc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvF23z-049sA",
        "outputId": "8d0ff989-ba74-47ae-8c6c-592e91ef5aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.62445\n",
            "Precision: 0.6260779002595036\n",
            "Recall: 0.62445\n",
            "F1-score: 0.619511326002176\n",
            "ROC-AUC: 0.7886096670714432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train a decision tree model\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred_dt = decision_tree_model.predict(X_val)\n",
        "\n",
        "# Evaluate the decision tree model\n",
        "accuracy_dt = accuracy_score(y_val, y_val_pred_dt)\n",
        "precision_dt = precision_score(y_val, y_val_pred_dt, average='weighted')\n",
        "recall_dt = recall_score(y_val, y_val_pred_dt, average='weighted')\n",
        "f1_dt = f1_score(y_val, y_val_pred_dt, average='weighted')\n",
        "roc_auc_dt = roc_auc_score(y_val, decision_tree_model.predict_proba(X_val), multi_class='ovr')\n",
        "\n",
        "# Show the evaluation metrics for decision tree\n",
        "print(f'Decision Tree - Accuracy: {accuracy_dt}')\n",
        "print(f'Decision Tree - Precision: {precision_dt}')\n",
        "print(f'Decision Tree - Recall: {recall_dt}')\n",
        "print(f'Decision Tree - F1-score: {f1_dt}')\n",
        "print(f'Decision Tree - ROC-AUC: {roc_auc_dt}')\n",
        "\n",
        "# Train a random forest model\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "random_forest_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred_rf = random_forest_model.predict(X_val)\n",
        "\n",
        "# Evaluate the random forest model\n",
        "accuracy_rf = accuracy_score(y_val, y_val_pred_rf)\n",
        "precision_rf = precision_score(y_val, y_val_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_val, y_val_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_val, y_val_pred_rf, average='weighted')\n",
        "roc_auc_rf = roc_auc_score(y_val, random_forest_model.predict_proba(X_val), multi_class='ovr')\n",
        "\n",
        "# Show the evaluation metrics for random forest\n",
        "print(f'Random Forest - Accuracy: {accuracy_rf}')\n",
        "print(f'Random Forest - Precision: {precision_rf}')\n",
        "print(f'Random Forest - Recall: {recall_rf}')\n",
        "print(f'Random Forest - F1-score: {f1_rf}')\n",
        "print(f'Random Forest - ROC-AUC: {roc_auc_rf}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsZ_0vpd5DCs",
        "outputId": "2ad34b04-e548-412b-fbaa-368ff6e778a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree - Accuracy: 0.6974\n",
            "Decision Tree - Precision: 0.697572397129452\n",
            "Decision Tree - Recall: 0.6974\n",
            "Decision Tree - F1-score: 0.6974741273266262\n",
            "Decision Tree - ROC-AUC: 0.7527277657734691\n",
            "Random Forest - Accuracy: 0.78065\n",
            "Random Forest - Precision: 0.7803768541921579\n",
            "Random Forest - Recall: 0.78065\n",
            "Random Forest - F1-score: 0.7804956645158008\n",
            "Random Forest - ROC-AUC: 0.9050128412352693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Prepare the test data (following similar steps as for training data)\n",
        "X_test = test_df_cleaned.drop(columns=['ID', 'Customer_ID', 'Name', 'SSN', 'Month'])\n",
        "\n",
        "# Label encode high cardinality features in test data\n",
        "for col, encoder in label_encoders.items():\n",
        "    # If there are categories in the test set that were not seen in the training set, replace them with a special value\n",
        "    X_test[col] = X_test[col].map(lambda s: '<unknown>' if s not in encoder.classes_ else s)\n",
        "    encoder.classes_ = np.append(encoder.classes_, '<unknown>')\n",
        "    X_test[col] = encoder.transform(X_test[col])\n",
        "\n",
        "# One-hot encode low cardinality features in test data\n",
        "X_test_encoded_low_cardinality = one_hot_encoder.transform(X_test[low_cardinality_features])\n",
        "X_test_encoded_low_cardinality_df = pd.DataFrame(X_test_encoded_low_cardinality, columns=one_hot_encoder.get_feature_names_out(low_cardinality_features))\n",
        "X_test = pd.concat([X_test.drop(columns=low_cardinality_features).reset_index(drop=True), X_test_encoded_low_cardinality_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Ensure no missing values are left\n",
        "X_test.fillna(X_test.median(), inplace=True)\n",
        "\n",
        "# Normalize numerical features in test data\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Use the best model (e.g., Random Forest) to predict on test data\n",
        "y_test_pred = random_forest_model.predict(X_test_normalized)\n",
        "\n",
        "# Output the predictions\n",
        "predictions = pd.DataFrame({'ID': test_df['ID'], 'Predicted_Credit_Score': y_test_pred})\n",
        "predictions.to_csv('predicted_credit_scores.csv', index=False)\n",
        "print(\"Predictions saved to 'predicted_credit_scores.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3ZFjWnu5IIv",
        "outputId": "807f3d34-5303-4733-ba55-6a164a961279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to 'predicted_credit_scores.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "teYF08dP6-2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}